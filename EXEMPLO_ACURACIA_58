import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Subset
from sklearn.model_selection import train_test_split
import random, numpy as np, matplotlib.pyplot as plt, time

torch.backends.cudnn.benchmark = True

# ========= 1. Espaço de Hiperparâmetros =========
space = {
    "learning_rate": [1e-2, 5e-3, 1e-3, 5e-4],
    "batch_size": [8, 16, 32],
    "n_filters": [16, 32, 64],
    "n_fc": [64, 128, 256],
    "dropout": [0.0, 0.25, 0.5]
}

# ========= 2. Carregamento de Dados =========
def load_data():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)
    train_idx, val_idx = train_test_split(range(len(dataset)), test_size=0.2, random_state=42)
    return Subset(dataset, train_idx[:4000]), Subset(dataset, val_idx[:1000]), Subset(dataset, val_idx)

trainset, valset, full_valset = load_data()

# ========= 3. CNN =========
class SmallCNN(nn.Module):
    def __init__(self, n_filters, n_fc, dropout):
        super().__init__()
        self.conv1 = nn.Conv2d(3, n_filters, 3, padding=1)
        self.conv2 = nn.Conv2d(n_filters, n_filters * 2, 3, padding=1)
        self.conv3 = nn.Conv2d(n_filters * 2, n_filters * 4, 3, padding=1)
        self.pool = nn.MaxPool2d(2, 2)
        self.dropout = nn.Dropout(dropout)
        self.fc1 = self.fc2 = None
        self.n_fc = n_fc

    def build(self, device):
        with torch.no_grad():
            x = torch.zeros(1, 3, 32, 32).to(device)
            x = self.pool(torch.relu(self.conv1(x)))
            x = self.pool(torch.relu(self.conv2(x)))
            x = self.pool(torch.relu(self.conv3(x)))
            x = self.dropout(x)
            flatten_dim = x.view(1, -1).shape[1]
        self.fc1 = nn.Linear(flatten_dim, self.n_fc).to(device)
        self.fc2 = nn.Linear(self.n_fc, 10).to(device)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = self.pool(torch.relu(self.conv3(x)))
        x = self.dropout(x)
        x = x.view(x.size(0), -1)
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)
        return self.fc2(x)

# ========= 4. Avaliação =========
def evaluate_model(ind, device, epochs=5):
    train_loader = DataLoader(trainset, batch_size=ind["batch_size"], shuffle=True, num_workers=2)
    val_loader = DataLoader(valset, batch_size=ind["batch_size"], shuffle=False, num_workers=2)

    model = SmallCNN(ind["n_filters"], ind["n_fc"], ind["dropout"]).to(device)
    model.build(device)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=ind["learning_rate"])

    model.train()
    for _ in range(epochs):
        for xb, yb in train_loader:
            xb, yb = xb.to(device), yb.to(device)
            optimizer.zero_grad()
            loss = criterion(model(xb), yb)
            loss.backward()
            optimizer.step()

    model.eval()
    correct, total = 0, 0
    preds, labels = [], []
    with torch.no_grad():
        for xb, yb in val_loader:
            xb, yb = xb.to(device), yb.to(device)
            out = model(xb)
            _, predicted = torch.max(out, 1)
            correct += (predicted == yb).sum().item()
            total += yb.size(0)
            preds.extend(predicted.cpu().numpy())
            labels.extend(yb.cpu().numpy())

    return correct / total, np.array(preds), np.array(labels)

# ========= 5. Algoritmo Genético Otimizado =========
def criar_individuo():
    return {k: random.choice(v) for k, v in space.items()}

def crossover(p1, p2):
    return {k: random.choice([p1[k], p2[k]]) for k in space}

def mutar(ind):
    k = random.choice(list(space.keys()))
    ind[k] = random.choice(space[k])
    return ind

def algoritmo_genetico(pop_size=6, geracoes=10, taxa_mutacao=0.3, device='cpu'):
    populacao = [criar_individuo() for _ in range(pop_size)]
    historico = []
    melhor_ind = None
    melhor_acc = -1
    tempo_inicio = time.time()

    for g in range(geracoes):
        print(f"\n--- Geração {g+1} ---")
        # evaluate_model returns 3 values: accuracy, preds, labels
        # So avaliacoes will be a list of tuples (ind, accuracy, preds, labels)
        avaliacoes = [(ind, *evaluate_model(ind, device)) for ind in populacao]
        avaliacoes.sort(key=lambda x: x[1], reverse=True)
        historico.append([a[1] for a in avaliacoes[:4]])

        # avaliacoes[0] is the best individual's evaluation: (ind, acc, preds, labels)
        if avaliacoes[0][1] > melhor_acc:
            melhor_ind, melhor_acc, melhor_preds, melhor_labels = avaliacoes[0]

        print(f"Melhor da geração {g+1}: {avaliacoes[0][1]:.4f} | {avaliacoes[0][0]}")

        # Elitismo (top 2): Correctly unpack the 4-element tuple to get only the individual (ind)
        nova_pop = [ind for ind, acc, preds, labels in avaliacoes[:2]]
        while len(nova_pop) < pop_size:
            # Make sure there are at least two elements in nova_pop for random.sample
            # This should be true after elitism adds the top 2, as pop_size is 6.
            p1, p2 = random.sample(nova_pop, 2)
            filho = crossover(p1, p2)
            if random.random() < taxa_mutacao:
                filho = mutar(filho)
            nova_pop.append(filho)
        populacao = nova_pop

    tempo_total = time.time() - tempo_inicio
    return melhor_ind, melhor_acc, melhor_preds, melhor_labels, historico, tempo_total

# ========= 6. Visualização =========
def plot_accuracies(historico):
    plt.figure(figsize=(8, 5))
    for i in range(len(historico[0])):
        plt.plot([h[i] for h in historico], label=f"Indivíduo {i+1}")
    plt.xlabel("Geração")
    plt.ylabel("Acurácia dos melhores")
    plt.title("Evolução da acurácia")
    plt.legend()
    plt.grid()
    plt.show()

def show_stats(historico, tempo_total, melhor_ind, acc):
    print("\n========== RELATÓRIO FINAL ==========")
    print(f"Tempo total: {tempo_total:.1f} s")
    print(f"Melhor indivíduo: {melhor_ind}")
    print(f"Acurácia final: {acc:.4f}")
    acuracias = [a for h in historico for a in h]
    print(f"Média: {np.mean(acuracias):.4f} | Máx: {np.max(acuracias):.4f} | Mín: {np.min(acuracias):.4f}")

def plot_image_examples(full_valset, preds, labels, acertos=True, n=5):
    # Ensure labels is a numpy array for comparison with preds
    if not isinstance(labels, np.ndarray):
         labels = np.array(labels)

    idxs = np.where((preds == labels) if acertos else (preds != labels))[0][:n]
    if len(idxs) == 0: print("Nenhum exemplo encontrado."); return
    plt.figure(figsize=(12, 3))
    for i, idx in enumerate(idxs):
        img, label = full_valset[idx]
        img = img.permute(1,2,0) * 0.5 + 0.5
        plt.subplot(1, n, i+1)
        plt.imshow(img.numpy())
        # Use the value from the original labels list/array, not the unpacked `label` variable
        plt.title(f"Pred:{preds[idx]}\nTrue:{labels[idx]}")
        plt.axis('off')
    plt.suptitle("Acertos" if acertos else "Erros")
    plt.show()

# ========= 7. Execução =========
device = 'cuda' if torch.cuda.is_available() else 'cpu'
melhor_ind, acc, preds, labels, historico, tempo_total = algoritmo_genetico(
    pop_size=6, geracoes=10, taxa_mutacao=0.3, device=device
)

show_stats(historico, tempo_total, melhor_ind, acc)
plot_accuracies(historico)

print("\n5 exemplos ACERTADOS:")
# Pass the original full_valset to plot_image_examples
plot_image_examples(full_valset, preds, labels, acertos=True, n=5)

print("\n5 exemplos ERRADOS:")
# Pass the original full_valset to plot_image_examples
plot_image_examples(full_valset, preds, labels, acertos=False, n=5)
